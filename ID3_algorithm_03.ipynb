{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8af1dc-bf61-4d0a-8775-83645b557d02",
   "metadata": {},
   "source": [
    "# **Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample.** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b627fd-0916-4dbb-8b0e-a95921415cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: {'outlook': {'Outlook': 'Play Tennis', 'Overcast': 'Yes', 'Rain': {'wind': {'Strong': 'No', 'Weak': 'Yes'}}, 'Sunny': {'humidity': {'High': 'No', 'Normal': 'Yes'}}}}\n",
      "The prediction accuracy is: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the dataset and define the feature as well as the target columns\n",
    "dataset = pd.read_csv('PlayTennis.csv', names=['outlook', 'temperature', 'humidity', 'wind', 'class'])\n",
    "\n",
    "# Define the attributes (excluding target 'class')\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind', 'PlayTennis']\n",
    "\n",
    "# Function to calculate the entropy of a dataset\n",
    "def entropy(target_col):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "    The only parameter is the target_col, which specifies the target column.\n",
    "    \"\"\"\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "# Function to calculate the Information Gain\n",
    "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a specific attribute.\n",
    "    \"\"\"\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "\n",
    "    # Calculate the values and corresponding counts for the split attribute\n",
    "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "\n",
    "    # Calculate the weighted entropy\n",
    "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * \n",
    "                               entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) \n",
    "                               for i in range(len(vals))])\n",
    "\n",
    "    # Calculate the information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "# Function to implement the ID3 algorithm\n",
    "def ID3(data, originaldata, features, target_attribute_name=\"class\", parent_node_class=None):\n",
    "    \"\"\"\n",
    "    Implement the ID3 algorithm to build a decision tree.\n",
    "    \"\"\"\n",
    "    # Stopping criteria:\n",
    "    \n",
    "    # If all target values have the same value, return that value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "\n",
    "    # If the dataset is empty, return the mode target feature value of the original dataset\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
    "\n",
    "    # If there are no more features to split on, return the parent node class\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "\n",
    "    else:\n",
    "        # Set the default value for this node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "\n",
    "        # Select the feature with the best information gain\n",
    "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "\n",
    "        # Create the tree structure (root node)\n",
    "        tree = {best_feature: {}}\n",
    "\n",
    "        # Remove the best feature from the feature set\n",
    "        features = [i for i in features if i != best_feature]\n",
    "\n",
    "        # Grow a branch for each possible value of the feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "            # Call ID3 algorithm recursively for each subset of the dataset\n",
    "            subtree = ID3(sub_data, originaldata, features, target_attribute_name, parent_node_class)\n",
    "\n",
    "            # Add the subtree to the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "# Function to predict the class label of a query using the decision tree\n",
    "def predict(query, tree, default=1):\n",
    "    \"\"\"\n",
    "    Predict the class label for a given query instance using the decision tree.\n",
    "    \"\"\"\n",
    "    for key in query.keys():\n",
    "        if key in tree.keys():\n",
    "            try:\n",
    "                result = tree[key][query[key]]\n",
    "            except:\n",
    "                return default\n",
    "\n",
    "            # Recursively predict if result is a subtree\n",
    "            if isinstance(result, dict):\n",
    "                return predict(query, result)\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "# Function to split the dataset into training data\n",
    "def train_test_split(dataset):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing data.\n",
    "    \"\"\"\n",
    "    training_data = dataset.iloc[:14].reset_index(drop=True)\n",
    "    return training_data\n",
    "\n",
    "# Function to test the prediction accuracy of the decision tree\n",
    "def test(data, tree):\n",
    "    \"\"\"\n",
    "    Test the decision tree's accuracy on the dataset.\n",
    "    \"\"\"\n",
    "    # Create query instances (remove the target column and convert to dictionary format)\n",
    "    queries = data.iloc[:, :-1].to_dict(orient=\"records\")\n",
    "\n",
    "    # Create a DataFrame to store predictions\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"])\n",
    "\n",
    "    # Calculate predictions and accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i, \"predicted\"] = predict(queries[i], tree, 1.0)\n",
    "\n",
    "    accuracy = (np.sum(predicted[\"predicted\"] == data[\"class\"]) / len(data)) * 100\n",
    "    print('The prediction accuracy is:', accuracy, '%')\n",
    "\n",
    "# Train the decision tree, display it, and test its accuracy\n",
    "training_data = train_test_split(dataset)\n",
    "tree = ID3(training_data, training_data, training_data.columns[:-1])\n",
    "print('Decision Tree:', tree)\n",
    "test(training_data, tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1051cd3-53b4-463f-9f27-7cb71d5150f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
